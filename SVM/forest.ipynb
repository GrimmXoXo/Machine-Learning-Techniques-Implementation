{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('forestfires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month            0\n",
       "monthsep         0\n",
       "monthoct         0\n",
       "monthnov         0\n",
       "monthmay         0\n",
       "monthmar         0\n",
       "monthjun         0\n",
       "monthjul         0\n",
       "monthjan         0\n",
       "monthfeb         0\n",
       "monthdec         0\n",
       "monthaug         0\n",
       "monthapr         0\n",
       "daywed           0\n",
       "daytue           0\n",
       "daythu           0\n",
       "daysun           0\n",
       "daysat           0\n",
       "daymon           0\n",
       "dayfri           0\n",
       "area             0\n",
       "rain             0\n",
       "wind             0\n",
       "RH               0\n",
       "temp             0\n",
       "ISI              0\n",
       "DC               0\n",
       "DMC              0\n",
       "FFMC             0\n",
       "day              0\n",
       "size_category    0\n",
       "siz              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sort_values(ascending=True)# No null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['siz'] = data['size_category'].str.strip().map({'small': 0, 'large': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "512    1\n",
       "513    1\n",
       "514    1\n",
       "515    0\n",
       "516    0\n",
       "Name: siz, Length: 517, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['siz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models and preprocessing the data using Scaling and OHE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in X: 517\n",
      "Number of samples in y: 517\n",
      "Number of samples in X_train: 413\n",
      "Number of samples in X_test: 104\n",
      "Number of samples in y_train: 413\n",
      "Number of samples in y_test: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gm/Desktop/Code/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:228: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Map 'Small' and 'Large' to 0 and 1\n",
    "data['siz'] = data['size_category'].map({'small': 0, 'large': 1})\n",
    "\n",
    "# Select features and target variable\n",
    "X = data.drop(['size_category', 'siz'], axis=1)  # Remove 'size_category' and 'siz' columns\n",
    "y = data['siz']\n",
    "\n",
    "# Handle missing values in the target variable (y)\n",
    "y = y.dropna()\n",
    "\n",
    "# Check the number of samples before the split\n",
    "print(\"Number of samples in X:\", len(X))\n",
    "print(\"Number of samples in y:\", len(y))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)#using stratify to reduce bias during training\n",
    "\n",
    "# Check the number of samples after the split\n",
    "print(\"Number of samples in X_train:\", len(X_train))\n",
    "print(\"Number of samples in X_test:\", len(X_test))\n",
    "print(\"Number of samples in y_train:\", len(y_train))\n",
    "print(\"Number of samples in y_test:\", len(y_test))\n",
    "\n",
    "# Define numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int', 'float']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply preprocessing steps consistently to both training and testing sets\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False,handle_unknown='ignore'), ['month']),\n",
    "    ])\n",
    "\n",
    "# Impute missing values if any\n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "X_train[numeric_features] = imputer.fit_transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = imputer.transform(X_test[numeric_features])\n",
    "\n",
    "# Standardize and one-hot encode the features\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train the SVM model using different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_model = SVC(kernel='linear')  # Using Linear kernel\n",
    "svm_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875\n",
      "Precision: 1.0\n",
      "Recall: 0.5357142857142857\n",
      "F1-score: 0.6976744186046512\n",
      "\n",
      "Confusion Matrix:\n",
      " [[76  0]\n",
      " [13 15]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92        76\n",
      "           1       1.00      0.54      0.70        28\n",
      "\n",
      "    accuracy                           0.88       104\n",
      "   macro avg       0.93      0.77      0.81       104\n",
      "weighted avg       0.89      0.88      0.86       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_model = SVC(kernel='sigmoid')  # Using sigmoid kernel gives similar results as linear model so the final would still be any of them\n",
    "svm_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7884615384615384\n",
      "Precision: 1.0\n",
      "Recall: 0.21428571428571427\n",
      "F1-score: 0.35294117647058826\n",
      "\n",
      "Confusion Matrix:\n",
      " [[76  0]\n",
      " [22  6]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.87        76\n",
      "           1       1.00      0.21      0.35        28\n",
      "\n",
      "    accuracy                           0.79       104\n",
      "   macro avg       0.89      0.61      0.61       104\n",
      "weighted avg       0.84      0.79      0.73       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF or Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_model = SVC(kernel='rbf')  # Using rbf kernel gives similar results as linear model so the final would still be any of them\n",
    "svm_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7980769230769231\n",
      "Precision: 1.0\n",
      "Recall: 0.25\n",
      "F1-score: 0.4\n",
      "\n",
      "Confusion Matrix:\n",
      " [[76  0]\n",
      " [21  7]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      1.00      0.88        76\n",
      "           1       1.00      0.25      0.40        28\n",
      "\n",
      "    accuracy                           0.80       104\n",
      "   macro avg       0.89      0.62      0.64       104\n",
      "weighted avg       0.84      0.80      0.75       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Grid Search CV to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "}\n",
    "\n",
    "# Create an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_svm_model.predict(X_test_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9423076923076923\n",
      "Precision: 0.9583333333333334\n",
      "Recall: 0.8214285714285714\n",
      "F1-score: 0.8846153846153847\n",
      "\n",
      "Confusion Matrix:\n",
      " [[75  1]\n",
      " [ 5 23]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96        76\n",
      "           1       0.96      0.82      0.88        28\n",
      "\n",
      "    accuracy                           0.94       104\n",
      "   macro avg       0.95      0.90      0.92       104\n",
      "weighted avg       0.94      0.94      0.94       104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Accuracy: The overall accuracy of the model has increased, indicating a higher proportion of correctly classified instances.\n",
    "   ### Precision: The positive class (1) precision is still high at 0.9583, indicating that when the model predicts positive, it is correct most of the time.\n",
    "   ###  Recall: The positive class recall has substantially increased to 0.8214, meaning the model is now identifying a larger proportion of the actual positive instances.\n",
    "   ### F1-score: The positive class F1-score has increased, indicating a better balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
